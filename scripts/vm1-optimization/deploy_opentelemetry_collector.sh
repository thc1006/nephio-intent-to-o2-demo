#!/bin/bash
# OpenTelemetry Collector éƒ¨ç½²è…³æœ¬ - 2025 å¹´æœ€ä½³å¯¦è¸
# /home/ubuntu/nephio-intent-to-o2-demo/scripts/vm1-optimization/deploy_opentelemetry_collector.sh

set -euo pipefail

readonly PROJECT_ROOT="/home/ubuntu/nephio-intent-to-o2-demo"
readonly MONITORING_NAMESPACE="monitoring"
readonly LOG_FILE="$PROJECT_ROOT/artifacts/otel-deployment-$(date +%Y%m%d-%H%M%S).log"

# Edge ç«™é»é…ç½®
readonly EDGE2_IP="172.16.0.89"
readonly EDGE1_IP="172.16.4.45"
readonly SLO_PORT="30090"
readonly O2IMS_PORT="31280"

# æ—¥èªŒå‡½æ•¸
log() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') [INFO] $*" | tee -a "$LOG_FILE"
}

log_error() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') [ERROR] $*" | tee -a "$LOG_FILE" >&2
}

# å‰µå»ºæ—¥èªŒç›®éŒ„
mkdir -p "$(dirname "$LOG_FILE")"

# æª¢æŸ¥ OpenTelemetry Collector äºŒé€²åˆ¶
check_otel_binary() {
    log "æª¢æŸ¥ OpenTelemetry Collector äºŒé€²åˆ¶..."

    if ! command -v otelcol-contrib &> /dev/null; then
        log "ä¸‹è¼‰ OpenTelemetry Collector äºŒé€²åˆ¶..."

        local temp_dir=$(mktemp -d)
        cd "$temp_dir"

        curl -LO "https://github.com/open-telemetry/opentelemetry-collector-releases/releases/download/v0.104.0/otelcol-contrib_0.104.0_linux_amd64.tar.gz"
        tar -xzf otelcol-contrib_*.tar.gz

        sudo mv otelcol-contrib /usr/local/bin/
        sudo chmod +x /usr/local/bin/otelcol-contrib

        cd - && rm -rf "$temp_dir"
        log "âœ… OpenTelemetry Collector äºŒé€²åˆ¶å®‰è£å®Œæˆ"
    else
        log "âœ… OpenTelemetry Collector äºŒé€²åˆ¶å·²å­˜åœ¨"
    fi
}

# å‰µå»ºé«˜ç´š OpenTelemetry é…ç½®
create_advanced_otel_config() {
    log "å‰µå»ºé«˜ç´š OpenTelemetry Collector é…ç½®..."

    mkdir -p "$PROJECT_ROOT/vm1-components"

    cat > "$PROJECT_ROOT/vm1-components/otel-collector-advanced.yaml" << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-advanced-config
  namespace: monitoring
data:
  otel-collector.yaml: |
    # 2025 å¹´é«˜ç´š OpenTelemetry é…ç½®
    receivers:
      # Prometheus å¤šç«™é»æ”¶é›†
      prometheus/edge2:
        config:
          scrape_configs:
          - job_name: 'edge2-slo-metrics'
            scrape_interval: 15s
            metrics_path: '/metrics/api/v1/slo'
            static_configs:
            - targets: ['172.16.0.89:30090']
              labels:
                site: 'edge2'
                cluster: 'vm-4-edge2'
                environment: 'production'
                region: 'region-1'
          - job_name: 'edge2-o2ims-metrics'
            scrape_interval: 30s
            metrics_path: '/o2ims/measurement/v1/metrics'
            static_configs:
            - targets: ['172.16.0.89:31280']
              labels:
                site: 'edge2'
                service: 'o2ims'
                environment: 'production'

      prometheus/edge1:
        config:
          scrape_configs:
          - job_name: 'edge1-slo-metrics'
            scrape_interval: 15s
            metrics_path: '/metrics/api/v1/slo'
            static_configs:
            - targets: ['172.16.4.45:30090']
              labels:
                site: 'edge1'
                cluster: 'vm-2-edge1'
                environment: 'production'
                region: 'region-1'

      # OTLP æ¥æ”¶å™¨ (traces and metrics)
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318

      # Jaeger æ¥æ”¶å™¨ (legacy traces)
      jaeger:
        protocols:
          grpc:
            endpoint: 0.0.0.0:14250
          thrift_http:
            endpoint: 0.0.0.0:14268

      # Host metrics æ”¶é›†
      hostmetrics:
        collection_interval: 30s
        scrapers:
          cpu:
          disk:
          load:
          filesystem:
          memory:
          network:
          process:

    processors:
      # æ‰¹è™•ç†å™¨
      batch:
        timeout: 10s
        send_batch_size: 1024

      # å…§å­˜é™åˆ¶å™¨
      memory_limiter:
        limit_mib: 1024
        spike_limit_mib: 256

      # è³‡æºè™•ç†å™¨ - 2025 èªç¾©ç´„å®š
      resource:
        attributes:
        - key: deployment.environment
          value: "production"
          action: upsert
        - key: service.namespace
          value: "nephio-multi-site"
          action: upsert
        - key: service.version
          value: "v2025.09"
          action: upsert
        - key: telemetry.sdk.version
          value: "0.104.0"
          action: upsert

      # å±¬æ€§è™•ç†å™¨ - æ¨™æº–åŒ–æ¨™ç±¤
      attributes:
        actions:
        - key: cluster
          action: update
          from_attribute: cluster
        - key: site
          action: update
          from_attribute: site

      # æŒ‡æ¨™è½‰æ›è™•ç†å™¨
      metricstransform:
        transforms:
        - include: slo_.*
          match_type: regexp
          action: update
          new_name: nephio_${1}

      # K8s å±¬æ€§è™•ç†å™¨
      k8sattributes:
        auth_type: "serviceAccount"
        passthrough: false
        filter:
          node_from_env_var: KUBE_NODE_NAME
        extract:
          metadata:
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.namespace.name
          - k8s.node.name
        pod_association:
        - sources:
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          - from: connection

    exporters:
      # Prometheus å°å‡ºå™¨ - 2025 æœ€ä½³å¯¦è¸
      prometheus:
        endpoint: "0.0.0.0:8889"
        enable_open_metrics: true
        add_metric_suffixes: true
        resource_to_telemetry_conversion:
          enabled: true

      # OTLP å°å‡ºå™¨ (for upstream aggregation)
      otlp/jaeger:
        endpoint: jaeger-collector:4317
        tls:
          insecure: true

      # Logging å°å‡ºå™¨ (debugging)
      logging:
        loglevel: info

      # File å°å‡ºå™¨ (backup)
      file:
        path: /tmp/otel-metrics.json

    extensions:
      # Health check
      health_check:
        endpoint: 0.0.0.0:13133

      # pprof for debugging
      pprof:
        endpoint: 0.0.0.0:1777

      # Memory ballast
      memory_ballast:
        size_mib: 512

    service:
      extensions: [health_check, pprof, memory_ballast]
      pipelines:
        metrics:
          receivers: [prometheus/edge2, prometheus/edge1, otlp, hostmetrics]
          processors: [memory_limiter, resource, attributes, metricstransform, k8sattributes, batch]
          exporters: [prometheus, logging, file]
        traces:
          receivers: [otlp, jaeger]
          processors: [memory_limiter, resource, k8sattributes, batch]
          exporters: [otlp/jaeger, logging]

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector-advanced
  namespace: monitoring
  labels:
    app: otel-collector
    version: advanced
spec:
  replicas: 2
  selector:
    matchLabels:
      app: otel-collector
      version: advanced
  template:
    metadata:
      labels:
        app: otel-collector
        version: advanced
    spec:
      serviceAccountName: otel-collector
      containers:
      - name: otel-collector
        image: otel/opentelemetry-collector-contrib:0.104.0
        command:
        - "/otelcol-contrib"
        - "--config=/etc/otel-collector-config/otel-collector.yaml"
        volumeMounts:
        - name: otel-collector-config-vol
          mountPath: /etc/otel-collector-config
        ports:
        - containerPort: 8889
          name: prometheus
        - containerPort: 4317
          name: otlp-grpc
        - containerPort: 4318
          name: otlp-http
        - containerPort: 14250
          name: jaeger-grpc
        - containerPort: 14268
          name: jaeger-http
        - containerPort: 13133
          name: health
        env:
        - name: KUBE_NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        resources:
          requests:
            memory: 512Mi
            cpu: 200m
          limits:
            memory: 1Gi
            cpu: 1000m
        livenessProbe:
          httpGet:
            path: /
            port: 13133
          initialDelaySeconds: 30
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /
            port: 13133
          initialDelaySeconds: 10
          periodSeconds: 10
      volumes:
      - name: otel-collector-config-vol
        configMap:
          name: otel-collector-advanced-config

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-collector
  namespace: monitoring

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: otel-collector
rules:
- apiGroups: [""]
  resources: ["pods", "nodes", "services", "endpoints", "namespaces"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["replicasets", "deployments", "daemonsets", "statefulsets"]
  verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: otel-collector
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: otel-collector
subjects:
- kind: ServiceAccount
  name: otel-collector
  namespace: monitoring

---
apiVersion: v1
kind: Service
metadata:
  name: otel-collector-advanced
  namespace: monitoring
  labels:
    app: otel-collector
spec:
  selector:
    app: otel-collector
    version: advanced
  ports:
  - name: prometheus
    port: 8889
    targetPort: 8889
  - name: otlp-grpc
    port: 4317
    targetPort: 4317
  - name: otlp-http
    port: 4318
    targetPort: 4318
  - name: jaeger-grpc
    port: 14250
    targetPort: 14250
  - name: jaeger-http
    port: 14268
    targetPort: 14268
  - name: health
    port: 13133
    targetPort: 13133
EOF

    log "âœ… é«˜ç´š OpenTelemetry Collector é…ç½®å‰µå»ºå®Œæˆ"
}

# éƒ¨ç½² OpenTelemetry Collector
deploy_otel_collector() {
    log "éƒ¨ç½² OpenTelemetry Collector..."

    # å‰µå»ºå‘½åç©ºé–“
    kubectl create namespace "$MONITORING_NAMESPACE" --dry-run=client -o yaml | kubectl apply -f -

    # éƒ¨ç½²é…ç½®
    if kubectl apply -f "$PROJECT_ROOT/vm1-components/otel-collector-advanced.yaml"; then
        log "âœ… OpenTelemetry Collector é…ç½®å·²æ‡‰ç”¨"
    else
        log_error "OpenTelemetry Collector é…ç½®æ‡‰ç”¨å¤±æ•—"
        return 1
    fi

    # ç­‰å¾…éƒ¨ç½²å°±ç·’
    log "ç­‰å¾… OpenTelemetry Collector å°±ç·’..."
    if kubectl wait --for=condition=available --timeout=300s deployment/otel-collector-advanced -n "$MONITORING_NAMESPACE"; then
        log "âœ… OpenTelemetry Collector éƒ¨ç½²æˆåŠŸ"
    else
        log_error "OpenTelemetry Collector éƒ¨ç½²è¶…æ™‚"
        kubectl describe deployment otel-collector-advanced -n "$MONITORING_NAMESPACE" >> "$LOG_FILE"
        return 1
    fi
}

# é©—è­‰ OpenTelemetry Collector
verify_otel_collector() {
    log "é©—è­‰ OpenTelemetry Collector..."

    # æª¢æŸ¥ Pod ç‹€æ…‹
    local pod_count=$(kubectl get pods -n "$MONITORING_NAMESPACE" -l app=otel-collector,version=advanced --field-selector=status.phase=Running | grep -c otel-collector || echo "0")
    if [[ "$pod_count" -ge 1 ]]; then
        log "âœ… OpenTelemetry Collector Pods é‹è¡Œæ­£å¸¸ ($pod_count å€‹)"
    else
        log_error "OpenTelemetry Collector Pods æœªæ­£å¸¸é‹è¡Œ"
        kubectl get pods -n "$MONITORING_NAMESPACE" -l app=otel-collector >> "$LOG_FILE"
        return 1
    fi

    # æª¢æŸ¥å¥åº·ç‹€æ…‹
    local pod_name=$(kubectl get pods -n "$MONITORING_NAMESPACE" -l app=otel-collector,version=advanced -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)
    if [[ -n "$pod_name" ]]; then
        if kubectl exec -n "$MONITORING_NAMESPACE" "$pod_name" -- curl -s http://localhost:13133 | grep -q "Server available"; then
            log "âœ… OpenTelemetry Collector å¥åº·æª¢æŸ¥é€šé"
        else
            log "WARNING: OpenTelemetry Collector å¥åº·æª¢æŸ¥å¤±æ•—"
        fi
    fi

    # æ¸¬è©¦æŒ‡æ¨™ç«¯é»
    log "æ¸¬è©¦ Prometheus æŒ‡æ¨™ç«¯é»..."
    kubectl port-forward -n "$MONITORING_NAMESPACE" service/otel-collector-advanced 8889:8889 &
    local pf_pid=$!
    sleep 10

    if curl -s --max-time 10 http://localhost:8889/metrics | grep -q "otelcol_process"; then
        log "âœ… Prometheus æŒ‡æ¨™ç«¯é»æ­£å¸¸"
    else
        log "WARNING: Prometheus æŒ‡æ¨™ç«¯é»å¯èƒ½æœ‰å•é¡Œ"
    fi

    kill $pf_pid 2>/dev/null || true
    wait $pf_pid 2>/dev/null || true
}

# æ¸¬è©¦é‚Šç·£ç«™é»é€£é€šæ€§
test_edge_connectivity() {
    log "æ¸¬è©¦é‚Šç·£ç«™é»é€£é€šæ€§..."

    # æ¸¬è©¦ Edge2 é€£æ¥
    if curl -s --max-time 10 "http://${EDGE2_IP}:${SLO_PORT}/health" &>/dev/null; then
        log "âœ… Edge2 (${EDGE2_IP}:${SLO_PORT}) é€£æ¥æ­£å¸¸"
    else
        log "WARNING: Edge2 (${EDGE2_IP}:${SLO_PORT}) é€£æ¥å¤±æ•—"
    fi

    # æ¸¬è©¦ Edge1 é€£æ¥ (å¦‚æœå¯ç”¨)
    if curl -s --max-time 10 "http://${EDGE1_IP}:${SLO_PORT}/health" &>/dev/null; then
        log "âœ… Edge1 (${EDGE1_IP}:${SLO_PORT}) é€£æ¥æ­£å¸¸"
    else
        log "INFO: Edge1 (${EDGE1_IP}:${SLO_PORT}) ä¸å¯ç”¨æˆ–æœªé…ç½®"
    fi

    # æ¸¬è©¦ Edge2 O2IMS
    if curl -s --max-time 10 "http://${EDGE2_IP}:${O2IMS_PORT}/o2ims/api/v1/health" &>/dev/null; then
        log "âœ… Edge2 O2IMS (${EDGE2_IP}:${O2IMS_PORT}) é€£æ¥æ­£å¸¸"
    else
        log "INFO: Edge2 O2IMS (${EDGE2_IP}:${O2IMS_PORT}) ä¸å¯ç”¨æˆ–æœªé…ç½®"
    fi
}

# é…ç½®å‘Šè­¦è¦å‰‡
setup_alerting() {
    log "é…ç½® OpenTelemetry å‘Šè­¦è¦å‰‡..."

    cat > "$PROJECT_ROOT/vm1-components/otel-alerts.yaml" << 'EOF'
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-alerting-rules
  namespace: monitoring
data:
  otel-alerts.yaml: |
    groups:
    - name: otel-collector
      rules:
      - alert: OtelCollectorDown
        expr: up{job="otel-collector"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "OpenTelemetry Collector is down"
          description: "OpenTelemetry Collector has been down for more than 5 minutes"

      - alert: OtelCollectorHighMemory
        expr: otelcol_process_memory_rss > 1000000000
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "OpenTelemetry Collector high memory usage"
          description: "OpenTelemetry Collector memory usage is above 1GB"

      - alert: EdgeSiteDown
        expr: up{site=~"edge.*"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Edge site {{ $labels.site }} is down"
          description: "Edge site {{ $labels.site }} has been unreachable for more than 2 minutes"
EOF

    kubectl apply -f "$PROJECT_ROOT/vm1-components/otel-alerts.yaml"
    log "âœ… OpenTelemetry å‘Šè­¦è¦å‰‡é…ç½®å®Œæˆ"
}

# ä¸»è¦åŸ·è¡Œå‡½æ•¸
main() {
    log "ğŸš€ é–‹å§‹éƒ¨ç½² OpenTelemetry Collector (2025 æœ€ä½³å¯¦è¸)..."

    check_otel_binary
    create_advanced_otel_config
    deploy_otel_collector
    verify_otel_collector
    test_edge_connectivity
    setup_alerting

    log "âœ… OpenTelemetry Collector éƒ¨ç½²å®Œæˆï¼"
    log "ğŸ“Š æŒ‡æ¨™ç«¯é»: http://localhost:8889/metrics"
    log "ğŸ¥ å¥åº·æª¢æŸ¥: http://localhost:13133"
    log "ğŸ“„ è©³ç´°æ—¥èªŒ: $LOG_FILE"

    # é¡¯ç¤ºéƒ¨ç½²ç‹€æ…‹
    kubectl get all -n "$MONITORING_NAMESPACE" -l app=otel-collector
}

# éŒ¯èª¤è™•ç†
trap 'log_error "OpenTelemetry Collector éƒ¨ç½²å¤±æ•—ï¼ŒæŸ¥çœ‹æ—¥èªŒ: $LOG_FILE"' ERR

main "$@"