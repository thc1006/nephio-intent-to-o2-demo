---
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
  labels:
    name: monitoring
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 30s
      evaluation_interval: 30s
      external_labels:
        cluster: 'smo-vm1'
        region: 'central'
        role: 'orchestrator'

    alerting:
      alertmanagers:
        - static_configs:
            - targets:
                - alertmanager:9093

    rule_files:
      - '/etc/prometheus/rules/*.yml'

    scrape_configs:
      # Local Kubernetes metrics
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
          - role: endpoints
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: default;kubernetes;https

      # Edge-1 (VM-2) monitoring
      - job_name: 'edge1-slo-service'
        static_configs:
          - targets: ['172.16.4.45:30090']
        metrics_path: '/metrics'
        scrape_interval: 15s
        relabel_configs:
          - target_label: site
            replacement: 'edge1'
          - target_label: service
            replacement: 'slo-monitoring'

      # Edge-2 (VM-4) monitoring
      - job_name: 'edge2-slo-service'
        static_configs:
          - targets: ['172.16.4.176:30090']
        metrics_path: '/metrics'
        scrape_interval: 15s
        relabel_configs:
          - target_label: site
            replacement: 'edge2'
          - target_label: service
            replacement: 'slo-monitoring'

      # Edge-1 O2IMS service
      - job_name: 'edge1-o2ims'
        static_configs:
          - targets: ['172.16.4.45:31280']
        metrics_path: '/metrics'
        scrape_interval: 30s
        scrape_timeout: 10s
        relabel_configs:
          - target_label: site
            replacement: 'edge1'
          - target_label: service
            replacement: 'o2ims'

      # Edge-2 O2IMS service
      - job_name: 'edge2-o2ims'
        static_configs:
          - targets: ['172.16.4.176:31280']
        metrics_path: '/metrics'
        scrape_interval: 30s
        scrape_timeout: 10s
        relabel_configs:
          - target_label: site
            replacement: 'edge2'
          - target_label: service
            replacement: 'o2ims'

      # GitOps health monitoring
      - job_name: 'gitea-health'
        static_configs:
          - targets: ['172.16.0.78:8888']
        metrics_path: '/api/healthz'
        scrape_interval: 60s
        relabel_configs:
          - target_label: service
            replacement: 'gitea'
          - target_label: component
            replacement: 'gitops'

      # Prometheus self-monitoring
      - job_name: 'prometheus'
        static_configs:
          - targets: ['localhost:9090']

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-rules
  namespace: monitoring
data:
  multi-site-alerts.yml: |
    groups:
      - name: multi-site-monitoring
        interval: 30s
        rules:
          # Edge site availability
          - alert: EdgeSiteUnreachable
            expr: up{job=~"edge[12]-.*"} == 0
            for: 2m
            labels:
              severity: critical
              component: infrastructure
            annotations:
              summary: "Edge site {{ $labels.site }} is unreachable"
              description: "Cannot reach {{ $labels.job }} for more than 2 minutes. This may indicate network issues or service failure."

          # SLO service monitoring
          - alert: SLOServiceDown
            expr: up{service="slo-monitoring"} == 0
            for: 1m
            labels:
              severity: critical
              component: slo
            annotations:
              summary: "SLO monitoring service down on {{ $labels.site }}"
              description: "SLO monitoring service on {{ $labels.site }} has been down for more than 1 minute"

          # O2IMS service monitoring
          - alert: O2IMSServiceDown
            expr: up{service="o2ims"} == 0
            for: 1m
            labels:
              severity: critical
              component: o2ims
            annotations:
              summary: "O2IMS service down on {{ $labels.site }}"
              description: "O2IMS service on {{ $labels.site }} has been down for more than 1 minute"

          # High latency detection
          - alert: HighLatencyDetected
            expr: |
              histogram_quantile(0.95,
                sum(rate(http_request_duration_seconds_bucket[5m])) by (le, site)
              ) > 1.0
            for: 5m
            labels:
              severity: warning
              component: performance
            annotations:
              summary: "High latency detected on {{ $labels.site }}"
              description: "95th percentile latency is {{ $value }}s on {{ $labels.site }}"

          # Error rate monitoring
          - alert: HighErrorRate
            expr: |
              (
                sum(rate(http_requests_total{status=~"5.."}[5m])) by (site)
                /
                sum(rate(http_requests_total[5m])) by (site)
              ) > 0.05
            for: 2m
            labels:
              severity: warning
              component: application
            annotations:
              summary: "High error rate on {{ $labels.site }}"
              description: "Error rate is {{ $value | humanizePercentage }} on {{ $labels.site }}"

          # GitOps sync monitoring
          - alert: GitOpsSyncFailure
            expr: up{service="gitea"} == 0
            for: 2m
            labels:
              severity: warning
              component: gitops
            annotations:
              summary: "GitOps sync service unavailable"
              description: "Gitea service is down, GitOps synchronization may be affected"

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: monitoring
  labels:
    app: prometheus
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      serviceAccountName: prometheus
      containers:
      - name: prometheus
        image: prom/prometheus:v2.40.0
        args:
          - '--config.file=/etc/prometheus/prometheus.yml'
          - '--storage.tsdb.path=/prometheus/'
          - '--web.console.libraries=/etc/prometheus/console_libraries'
          - '--web.console.templates=/etc/prometheus/consoles'
          - '--storage.tsdb.retention.time=30d'
          - '--web.enable-lifecycle'
          - '--web.external-url=http://172.16.0.78:31090'
        ports:
        - containerPort: 9090
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        volumeMounts:
        - name: prometheus-config-volume
          mountPath: /etc/prometheus/
        - name: prometheus-rules-volume
          mountPath: /etc/prometheus/rules/
        - name: prometheus-storage-volume
          mountPath: /prometheus/
      volumes:
      - name: prometheus-config-volume
        configMap:
          defaultMode: 420
          name: prometheus-config
      - name: prometheus-rules-volume
        configMap:
          defaultMode: 420
          name: prometheus-rules
      - name: prometheus-storage-volume
        emptyDir: {}

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: monitoring

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: monitoring

---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: monitoring
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9090"
spec:
  type: NodePort
  selector:
    app: prometheus
  ports:
    - name: web
      port: 9090
      targetPort: 9090
      nodePort: 31090
      protocol: TCP