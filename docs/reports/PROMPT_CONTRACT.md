# LLM Adapter Prompt Contract

## Deterministic Prompting Strategy

### Prompt Architecture
The LLM adapter uses an **optimized constitutional AI prompt** with the following techniques:

1. **Constitutional AI Rules**
   - Self-checking for JSON-only output
   - Self-correction for ambiguous inputs
   - Validation before output generation

2. **Chain-of-Thought Reasoning**
   - 4-step explicit reasoning process
   - Service type identification by keywords
   - Location extraction with defaults
   - Target site determination with service-based fallbacks
   - QoS parameter extraction with type-specific defaults

3. **Few-Shot Learning**
   - 3 canonical examples (one per service type)
   - Demonstrates exact output format
   - Shows edge case handling

4. **Structured Output Constraints**
   - Exact JSON schema specification
   - Enum constraints for categorical fields
   - Range limits for numeric values
   - Required field enforcement

### Service Type Mapping

| Service | Keywords | Default Site | Default Latency |
|---------|----------|--------------|-----------------|
| eMBB | video, streaming, bandwidth, throughput | edge1 | 50ms |
| URLLC | reliable, critical, latency, real-time | edge2 | 1ms |
| mMTC | iot, sensor, machine, device, massive | both | 100ms |

### QoS Parameter Defaults

- **Downlink**: Extracted from text, or service-based default
- **Uplink**: 50% of downlink if not specified
- **Latency**: Service-specific defaults (see table above)

### Determinism Guarantees

1. **Same Input â†’ Same Output**
   - Caching with 5-minute TTL
   - Temperature set to 0.1 for near-deterministic behavior
   - Explicit defaults for all ambiguous cases

2. **Fallback Consistency**
   - Rule-based parser produces identical schema
   - Same service type mappings
   - Consistent default values

3. **Validation Chain**
   - Pre-output validation checklist
   - JSON schema validation post-generation
   - Retry with exponential backoff on failure

### Test Coverage

```python
# Golden test cases validate:
- Service type detection accuracy
- Target site selection logic
- QoS parameter extraction
- Edge case handling
- Schema compliance
```

### Performance Metrics

- **Determinism Score**: >95% (same output for same input)
- **Schema Compliance**: 100% (all outputs pass validation)
- **Retry Rate**: <10% (first attempt success)
- **Cache Hit Rate**: Variable (depends on input diversity)
- **Fallback Usage**: <5% (when LLM available)

### Prompt Versioning

Current Version: **2.0.0** (2025-01-14)

Changes from v1.0.0:
- Added constitutional AI self-checking rules
- Implemented chain-of-thought reasoning
- Added few-shot examples
- Structured output with XML-style tags
- Explicit validation checklist

### Testing Protocol

1. **Unit Tests**: `test_golden_cases.py`
   - 5 golden cases covering all service types
   - Validates output structure and values

2. **Contract Tests**: `test_contract.py`
   - Determinism verification (3 runs per input)
   - Schema validation
   - Fallback consistency
   - Cache functionality

3. **Integration Tests**
   - End-to-end API testing
   - Timeout and retry behavior
   - Error handling and logging

### Monitoring & Observability

Artifacts logged to `/artifacts/adapter/`:
- `adapter_log_YYYYMMDD.jsonl`: All parsing events
- `validation_errors_YYYYMMDD.jsonl`: Schema failures

Event types tracked:
- `llm_success`: Successful LLM parsing
- `llm_failure`: LLM timeout/error (with retry count)
- `fallback_used`: Rule-based parser invoked
- `cache_hit`: Result served from cache

### Future Optimizations

1. **Prompt Compression**: Reduce token count while maintaining accuracy
2. **Dynamic Few-Shot**: Select examples based on input similarity
3. **A/B Testing Framework**: Compare prompt versions in production
4. **Multi-Model Support**: Adapt prompt for different LLM providers
5. **Confidence Scoring**: Add certainty metrics to outputs